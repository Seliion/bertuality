{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f588af81",
   "metadata": {},
   "source": [
    "# BERTuality\n",
    "\n",
    " <p style='text-align: justify;'>In this repository, we explore a way to check the topicality of textual data using the \"BERT\" language model and generate data corresponding to topicality. Our concept is called BERT-actuality (=BERTuality) and is a pipeline that is able to overcome the limitations of a pre-trained large language model by collecting correct data according to its topicality and predicting a correct value for a searched value.\n",
    "<br><br>\n",
    "<b>Why is this important?</b> There are over 6.6 million English-language Wikipedia articles on the Internet. In the USA alone, over 5,000 new news articles are published every day. Assuming that not every source is constantly updated, it can be stated that a large amount of outdated information exists. Timeliness in this context is understood as a property of the current relevance of information. Outdated information no longer corresponds to the current conditions of the real world, since it has become obsolete due to the passage of time. Outdated information thus poses a danger because it is misinformation in our case. The use of misinformation can lead to misunderstanding, economic or reputational damage. Therefore, it is of great interest to both individuals and businesses to obtain and use only current information. We describe a best practice for how BERT keeps information current.\n",
    "<br><br>\n",
    "<b>How do we achieve it?</b> BERTuality generates a current value for the [MASK] token for a sentence of the form \"Prime Minister [MASK] is the leader of Japan\". In this case, the result is the word \"Kishida\" (as of February 2023). To predict the word for the [MASK] token, the language model BERT, which was previously sensitized to the current state, is used. In the first step, BERTuality analyzes the outdated information and systematically extracts up-to-date information from various data sources. In the second step, this information is split into individual sentences and transformed into an optimal form for BERT. Procedures are presented that allow BERT to be made aware of timeliness using these sentences. In the third step, BERT is used to generate a correct and up-to-date prediction.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f47c1",
   "metadata": {},
   "source": [
    "# 1. Dependencies\n",
    "\n",
    "<p style='text-align: justify;'>To keep this notebook readable and simple, we will keep most of the code out of this notebook and use all the necessary functions from our bertuality package as we go through the chapters. If you need more information about the implementation, you will need to refer to the files in the bertuality package imported here. But for now, we will import all the necessary dependencies:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c4bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "from bertuality import BERTuality\n",
    "from bertuality import BERTuality_loader\n",
    "from bertuality import BERTuality_quickstart\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 18)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0fc8b",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>We also need to define a path to our BERTuality package and import our pre-saved and pre-trained model. This pre-trained model is equal to \"bert-base-uncased\". Although it is also possible to simply import models via Hugging Face, we have also saved them in the package files.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd18e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"bertuality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05fc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(os.path.join(path, \"model\"))\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(path, \"tokenizer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb01e4a",
   "metadata": {},
   "source": [
    "# 2. The Basics needed\n",
    "\n",
    "<p style='text-align: justify;'>The training data of BERT includes books with 800 million words and Wikipedia articles with 2.5 billion English words. In the original research work, the BERT core model is trained with two self-supervised learning tasks, including the Masked Language Model. In this pre-training method, some tokens are masked by [MASK] tokens. The model is then trained to predict the masked tokens by determining the context from the surrounding tokens.\n",
    "To predict the [MASK] tokens, we use a pipeline from the transformers library. Our goal is to use BERT to make both contextually correct and topical statements about a previously masked [MASK] token.</p>\n",
    "\n",
    "\n",
    "## 2.1 Problems with pre-trained models\n",
    "\n",
    "<p style='text-align: justify;'>To take a closer look at the predictions of a trained model, we first need to define a pipeline. This can be easily done with the Huggingface models and API's. The pipeline below can be used for so-called masekd predictions, where the [MASK] token is predicted by the model.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18d97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d78ba",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Below are the predictions for two [MASK] sentences, each containing a [MASK] token.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b704d254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paris is the capital of france.</td>\n",
       "      <td>0.950923</td>\n",
       "      <td>2605</td>\n",
       "      <td>f r a n c e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sequence     score  token    token_str\n",
       "0  paris is the capital of france.  0.950923   2605  f r a n c e"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediciton for our first sentence:\n",
    "\n",
    "test_sentence_1 = \"Paris is the capital of [MASK].\"\n",
    "prediction_1 = pre_trained_pipeline(test_sentence_1)\n",
    "\n",
    "prediction_1_df = pd.DataFrame(prediction_1)\n",
    "prediction_1_df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054ec75",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>The prediction for the first sentence is correct. Since this information is not related to actuality, it does not change over time. So we can say that the BERT \"knows\" this information based on its training data and predicts correctly.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a9ecb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tim cook is the ceo of amazon.</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>9733</td>\n",
       "      <td>a m a z o n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sequence     score  token    token_str\n",
       "0  tim cook is the ceo of amazon.  0.007595   9733  a m a z o n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediciton for our second sentence:\n",
    "\n",
    "test_sentence_2 = \"Tim Cook is the CEO of [MASK].\"\n",
    "prediction_2 = pre_trained_pipeline(test_sentence_2)\n",
    "\n",
    "prediction_2_df = pd.DataFrame(prediction_2)\n",
    "prediction_2_df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246df87",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>However, this is not always the case. If a piece of information relates to time, it may become obsolete due to the lapse of time. For example, the data used to train BERT may become obsolete after training. Consequently, BERT then has an outdated knowledge state and cannot make the current prediction. In addition, some information is not included in the training data of the trained BERT model, so no actual prediction can be made.\n",
    "<br><br>\n",
    "In order to get a correct prediction also for update related information, BERT needs to be made aware of updates. To this end, we will look at a way to accomplish this.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed412f09",
   "metadata": {},
   "source": [
    "## 2.2 Approaches to create a sensitivity to the topicallity\n",
    "\n",
    "<p style='text-align: justify;'>There are several approaches to making BERT aware of a current context. Essentially, the goal is to \"teach\" BERT new content that is unknown due to the time and topic constraints of the training data. In our case of actuality awareness, BERT is taught the current state of actuality-related information.\n",
    "<br><br>\n",
    "One of these approaches is called Priming and is the idea that exposure to one stimulus may influence a response to a subsequent stimulus, without conscious guidance or intention. The priming effect refers to the positive or negative effect of a rapidly presented stimulus (priming stimulus) on the processing of a second stimulus (target stimulus) that appears shortly after.\n",
    "<br><br>\n",
    "Priming can also be used with BERT to influence the prediction of a [MASK] token. To do this, the word or phrase that is to be used to influence BERT is concatenated with the [MASK] phrase. In the case of topicality sensitization, the priming may look like the following:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc18b33d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tim cook is the ceo of apple. he has been the chief executive officer of apple inc. since 2011.</td>\n",
       "      <td>0.941625</td>\n",
       "      <td>6207</td>\n",
       "      <td>a p p l e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          sequence  \\\n",
       "0  tim cook is the ceo of apple. he has been the chief executive officer of apple inc. since 2011.   \n",
       "\n",
       "      score  token  token_str  \n",
       "0  0.941625   6207  a p p l e  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primed prediction for our second sentence with additional information\n",
    "\n",
    "add_information = \"He has been the chief executive officer of Apple Inc. since 2011.\"\n",
    "\n",
    "primed_sentence =  test_sentence_2 + \" \" + add_information\n",
    "primed_prediction = pre_trained_pipeline(primed_sentence)\n",
    "\n",
    "primed_prediction_df = pd.DataFrame(primed_prediction)\n",
    "primed_prediction_df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e06d87",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>It is shown that concatenating additional information (=input sentences) with the [MASK] sentence affects the prediction of the [MASK] token. Without priming, BERT predicted that Tim Cook is the CEO of Amazon. With priming, Apple is predicted. This principle of priming can also influence the prediction of BERT just by the presence of individual words before or after the [MASK] sentence.</p>\n",
    "<p style='text-align: justify;'>It follows that the knowledge of the BERT can be influenced by adding more information. In the following, we will use this principle for our BERTuality pipeline, which will be able to predict current information without having a current knowledge base.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122c286",
   "metadata": {},
   "source": [
    "# 3. BERTuality\n",
    "\n",
    "<p style='text-align: justify;'>In this chapter, we will go over the BERTuality pipeline and introduce its structure and the functionality of the individual modules. This is followed by a schematic overview of the process:</p>\n",
    "\n",
    "\n",
    "<img src=\"references/bertuality_pipeline.jpg\" width=\"60%\">\n",
    "\n",
    "<p style='text-align: justify;'><b>Description:</b> First, the masked sentence is passed to the pipeline to extract all relevant keywords from it. Based on the keywords, relevant data is searched in an arbitrary data source. Then, the found input data passes through the text data preparation and query module to filter out optimal input sentences. Based on these prepared input sentences, a suitable and up-to-date token for the [MASK] token is predicted for the masked sentence.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a8a2e",
   "metadata": {},
   "source": [
    "## 3.1 Keyword Extraction\n",
    "\n",
    "<p style='text-align: justify;'>First, the keywords are extracted from the [MASK] sentence, which will be used in the next steps to search for the relevant information. For an optimal search it is of interest that the list of keywords contains only the most relevant words.\n",
    "<br><br>\n",
    "To obtain an optimal keyword extraction, the part-of-speech method is used for this purpose. POS tagging is the assignment of a POS tag to each word according to its definition and context. The various POS tags correspond to Penn Treebank's tag set, which includes various tags for word types such as verbs, adjectives, and nouns.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e382a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple test sentence for exploring the bertuality pipeline\n",
    "\n",
    "mask_sentence = \"Prime minister [MASK] is the leader of Japan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5b04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keywords of \"Prime minister [MASK] is the leader of Japan.\" are ['Prime', 'minister', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "keywords = BERTuality.pos_keywords(mask_sentence)\n",
    "\n",
    "print('The keywords of \"{}\" are {}'.format(mask_sentence, keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ec888",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Another possibility to extract relevant keywords from the masked sentence would be Named Entity Recognition, but this will not be discussed further here. Due to the very good results for us, the POS method for BERTuality is used in the following.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a662ba8",
   "metadata": {},
   "source": [
    "## 3.2 Loading the data\n",
    "\n",
    "<p style='text-align: justify;'>As mentioned before, the basic idea of BERTuality is that BERT is primed using temporally current input data to make a correct prediction for the [MASK] token. Thus, the quality of a prediction depends in particular on the data quality of a source. The reason for this is the later extraction of optimal sentences from the data source, which are used as input for BERT. At the moment, the BERTuality pipeline uses three freely available data sources from the Internet.</p>\n",
    "\n",
    "These are APIs for: \n",
    "    <ul>\n",
    "        <li><b>NewsAPI</b>, which primarily provides short descriptions of articles from over 80,000 sources, mostly one to two sentences in length.</li>\n",
    "        <li><b>TheGuardian</b>, which offers detailed and longer journalistic articles with many sentences on current topics.</li>\n",
    "        <li><b>Wikipedia</b>, which serves as a backup in case no current news articles can be found on a particular topic</li>\n",
    "    </ul>\n",
    "\n",
    "<p style='text-align: justify;'>The keywords generated by POS tagging are concatenated into a single string using AND operators in this step. The created string is passed as a search query to a function that coordinates the search across the different APIs. Afterwards, the text data is extracted from all search results of the found sources with the help of web scrapers. The quality of the selected keywords is important here, as suitable sources are searched exclusively on the basis of them. In this step, special care must be taken to ensure that only current data from a specific time period is obtained.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c00abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Api's can be called individually with their own function, but for simplicity we will call the general news_loader function:\n",
    "\n",
    "textual_data = BERTuality_loader.news_loader(from_date=\"2023-02-01\", \n",
    "                                             to_date=\"2023-03-01\", \n",
    "                                             keywords=keywords, \n",
    "                                             use_NewsAPI=True, \n",
    "                                             use_guardian=True, \n",
    "                                             use_wikipedia=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc3e4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 articles for NewsApi:\n",
      "\n",
      "Article Nr. 0:\n",
      "JAXA second attempt at launching the H3 rocket has ended up becoming a major setback for Japan space ambitions. While the rocket was able to leave the launch pad the country space authorities were forced to activate its flight termination system a few.\n",
      "\n",
      "Article Nr. 1:\n",
      "BBC correspondents explain why historic rivals are trying to rebuild trust and who stands to gain.\n",
      "\n",
      "Article Nr. 2:\n",
      "The number of births registered in Japan plummeted to another record low last year the latest worrying statistic in a decadeslong decline that the country authorities have failed to reverse despite their extensive efforts.\n"
     ]
    }
   ],
   "source": [
    "# Information about NewsAPI:\n",
    "\n",
    "news_api = textual_data[0]\n",
    "\n",
    "print(f\"Found {len(news_api)} articles for NewsApi:\")\n",
    "      \n",
    "show_articles = 3   \n",
    "for i in range(show_articles):\n",
    "    print(f\"\\nArticle Nr. {i}:\")\n",
    "    print(news_api[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a552d30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 articles for The Guardian:\n",
      "\n",
      "Article Nr. 0:\n",
      "Officials meet in Tokyo to discuss concerns at China cooperation with Russia and Japan military buildup Chinese and Japanese officials met in Tokyo on Wednesday for formal security talks for the first time in four years in a meeting aimed at stabilising increasingly strained relations. In Japan national security strategy released in December China was described as the greatest strategic challenge to Japan peace and security. Both sides expressed concerns at Wednesday meeting. China said it was troubled by Japan military buildup while Tokyo is worried about China suspected use of spy balloons as well as Chinese  ...\n"
     ]
    }
   ],
   "source": [
    "# Information about The Guardian:\n",
    "\n",
    "guardian = textual_data[1]\n",
    "\n",
    "print(f\"Found {len(guardian)} articles for The Guardian:\")\n",
    "     \n",
    "show_articles = 1\n",
    "show_percentage = 0.2\n",
    "for i in range(show_articles):\n",
    "    print(f\"\\nArticle Nr. {i}:\")\n",
    "    print(guardian[i][:int(len(guardian[i]) * show_percentage)], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff160954",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 article for Wikipedia:\n",
      "\n",
      "Article Nr. 0:\n",
      "The Prime Minister of Japan is the chief minister of the government of Japan and the head of the Japanese Cabinet. This is a list of prime ministers of Japan from when the first Japanese prime minister Itō Hirobumi took office in 1885 until the present day. 32 prime ministers under the Meiji Constitution had a mandate from the Emperor. The electoral mandates shown are for the House of Representatives lower house of the Imperial Diet that was not constitutionally guaranteed to have any influence on the appointment of the prime minister. Currently the prime minister under the Constitution of Japan shall be designated from among the members of the National Diet and shall be appointed by the Emperor after being nominated by the National Diet. The incumbent prime minister is Fumio Kishida.\n"
     ]
    }
   ],
   "source": [
    "# Information about Wikipedia - Wikipedia articles contain only the description \n",
    "\n",
    "wikipedia = textual_data[2]\n",
    "\n",
    "print(f\"Found {len(wikipedia)} article for Wikipedia:\")\n",
    "      \n",
    "show_articles = 1  \n",
    "for i in range(show_articles):\n",
    "    print(f\"\\nArticle Nr. {i}:\")\n",
    "    print(wikipedia[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c02c2a",
   "metadata": {},
   "source": [
    "## 3.3 Preparing the data\n",
    "\n",
    "<p style='text-align: justify;'>In order to use the collected data, it is important to put it into a consistent, clean form that is useful for BERT. The reason for this is contamination of texts, which can affect subsequent predictions for the [MASK] token. In particular, the cleanup of texts is done by a function of the pipeline, which consists of a variety of regular expressions that filter out or transform various anomalies and impurities. Regular expressions are particularly suitable for textual data for data preparation, as it is possible to specify a uniform syntax of texts through these rules. All impurities, such as elements of page description languages, acronyms, abbreviations, and formatting errors that violate these rules are either replaced or transformed into a different form. This step is important because only a syntactically clean block of text can be broken down into error-free individual sentences and inserted into a list.</p>\n",
    "\n",
    "<p style='text-align: justify;'>Note: Due to the structure of the news_loader and for simplification reasons, this step is automatically performed by the text_clean_up function when loading the articles. </p>\n",
    "\n",
    "<p style='text-align: justify;'>For further preparation, we also need to split the found articles into sentences and merge them into a one-dimensional list. For this step we will use the split and merge functions from our package. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a12855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 857 possible input sentences.\n"
     ]
    }
   ],
   "source": [
    "split_data = BERTuality.nltk_sentence_split(textual_data)\n",
    "merged_data = BERTuality.merge_sentences(split_data)\n",
    "\n",
    "print(f\"Found a total of {len(merged_data)} possible input sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb05e1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Merged API Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JAXA second attempt at launching the H3 rocket has ended up becoming a major setback for Japan s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>While the rocket was able to leave the launch pad the country space authorities were forced to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BBC correspondents explain why historic rivals are trying to rebuild trust and who stands to gain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The number of births registered in Japan plummeted to another record low last year the latest wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The longrange missile is Pyongyang fourth round of launches in a week ahead of a crucial summit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>This is a list of prime ministers of Japan from when the first Japanese prime minister Itō Hirob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>32 prime ministers under the Meiji Constitution had a mandate from the Emperor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>The electoral mandates shown are for the House of Representatives lower house of the Imperial Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Currently the prime minister under the Constitution of Japan shall be designated from among the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>The incumbent prime minister is Fumio Kishida.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    Merged API Sentences\n",
       "0    JAXA second attempt at launching the H3 rocket has ended up becoming a major setback for Japan s...\n",
       "1    While the rocket was able to leave the launch pad the country space authorities were forced to a...\n",
       "2     BBC correspondents explain why historic rivals are trying to rebuild trust and who stands to gain.\n",
       "3    The number of births registered in Japan plummeted to another record low last year the latest wo...\n",
       "4       The longrange missile is Pyongyang fourth round of launches in a week ahead of a crucial summit.\n",
       "..                                                                                                   ...\n",
       "852  This is a list of prime ministers of Japan from when the first Japanese prime minister Itō Hirob...\n",
       "853                      32 prime ministers under the Meiji Constitution had a mandate from the Emperor.\n",
       "854  The electoral mandates shown are for the House of Representatives lower house of the Imperial Di...\n",
       "855  Currently the prime minister under the Constitution of Japan shall be designated from among the ...\n",
       "856                                                       The incumbent prime minister is Fumio Kishida.\n",
       "\n",
       "[857 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below are all found sentences from our data sources\n",
    "\n",
    "api_sentences = pd.DataFrame(merged_data, columns=[\"Merged API Sentences\"])\n",
    "api_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe33ec8",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Theoretically, it is already possible from this step to pass the list of collected sentences to BERT and to generate a prediction for the searched [MASK] token for each individual sentence. However, this should not be done, since at this point all sentences from the data sources are unfiltered. These unfiltered sentences may contain irrelevant information that worsens the prediction and drastically increases the pipeline processing time. Therefore, the amount of sentences must be reduced to optimal sentences in the next step of the pipeline.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e125aba",
   "metadata": {},
   "source": [
    "## 3.4 Finding optimal sentences\n",
    "\n",
    "<p style='text-align: justify;'>In addition to cleaning through data preparation, the query module from this chapter offers the possibility of filtering for optimal data and modifying it if necessary. The goal is to reduce the amount of sentences to optimal input sentences. This should on the one hand increase the performance and on the other hand improve the quality of the prediction.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7169feb",
   "metadata": {},
   "source": [
    "### 3.4.1 Extraction Query\n",
    "\n",
    "<p style='text-align: justify;'>We introduce the extraction query into the query module to remove irrelevant sentences from the dataset obtained so far. Only sentences that contain the respective keywords are considered. However, not all keywords must be contained in a sentence, otherwise this leads to a too strict data filtering.</p>\n",
    "\n",
    "<p style='text-align: justify;'>With a minimum number of keywords defined by the subset_size variable, subsets are formed. The subsets contain sentences with partially existing keywords. A sentence can contain variations of a keyword. If none of the keywords are present in a sentence, the sentence is removed from the set of input sentences.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "485943a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a subset_size\n",
    "\n",
    "subset_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1cecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:      ['Prime', 'minister', 'Japan']\n",
      "\n",
      "   0. Subset = ['Prime', 'minister']\n",
      "   1. Subset = ['Prime', 'Japan']\n",
      "   2. Subset = ['minister', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "# and have a look at the created subsets of our keywords\n",
    "\n",
    "subsets = BERTuality.create_subsets(keywords, subset_size=subset_size)\n",
    "\n",
    "print(f\"Keywords:      {keywords}\\n\")\n",
    "for i, j in enumerate(subsets):\n",
    "    print(f\"   {i}. Subset = {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a04efdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Query:\n",
      "-> Found a total of 80 input sentences.\n",
      "-> Reduced the amount of sentences by 90.67 %\n"
     ]
    }
   ],
   "source": [
    "# call the extraction query with the defined subset_size and filter for more optimal sentences \n",
    "\n",
    "extraction_query = BERTuality.filter_for_keyword_subsets(input_sentences=merged_data, \n",
    "                                                         keywords=keywords, \n",
    "                                                         tokenizer=tokenizer, \n",
    "                                                         subset_size=subset_size, \n",
    "                                                         duplicates=False)\n",
    "\n",
    "print(\"Extraction Query:\")\n",
    "print(f\"-> Found a total of {len(extraction_query)} input sentences.\")\n",
    "print(f\"-> Reduced the amount of sentences by {100 - len(extraction_query) * 100 / len(merged_data):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a23c0ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extraction Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korean President Yoon Suk Yeol departed South Korea on Thursday for Tokyo to meet with Pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The other toasted a close friendship with MoscowOn Tuesday Japan prime minister laid a wreath fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese Prime Minister Fumio Kishida began a surprise visit to Ukraine early Tuesday hours afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>College football Mexican cola and muffins United Kingdom prime minister has plenty to talk about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sen. Mike Lee of Utah demanded that Prime Minister Fumio Kishida release a U. S. Navy lieutenant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>The electoral mandates shown are for the House of Representatives lower house of the Imperial Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Currently the prime minister under the Constitution of Japan shall be designated from among the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The incumbent prime minister is Fumio Kishida.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Prime Miniser Kishida is Japan first postwar leader to enter a war zone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>South Korea unification minister and senior Japanese government officials have reaffirmed their ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       Extraction Query\n",
       "0   South Korean President Yoon Suk Yeol departed South Korea on Thursday for Tokyo to meet with Pri...\n",
       "1   The other toasted a close friendship with MoscowOn Tuesday Japan prime minister laid a wreath fo...\n",
       "2   Japanese Prime Minister Fumio Kishida began a surprise visit to Ukraine early Tuesday hours afte...\n",
       "3   College football Mexican cola and muffins United Kingdom prime minister has plenty to talk about...\n",
       "4   Sen. Mike Lee of Utah demanded that Prime Minister Fumio Kishida release a U. S. Navy lieutenant...\n",
       "..                                                                                                  ...\n",
       "75  The electoral mandates shown are for the House of Representatives lower house of the Imperial Di...\n",
       "76  Currently the prime minister under the Constitution of Japan shall be designated from among the ...\n",
       "77                                                       The incumbent prime minister is Fumio Kishida.\n",
       "78                             Prime Miniser Kishida is Japan first postwar leader to enter a war zone.\n",
       "79  South Korea unification minister and senior Japanese government officials have reaffirmed their ...\n",
       "\n",
       "[80 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_df = pd.DataFrame(extraction_query, columns=[\"Extraction Query\"])\n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161177a",
   "metadata": {},
   "source": [
    "### 3.4.2 Similarity Query\n",
    "\n",
    "<p style='text-align: justify;'>We introduce the similarity query in the query module to reduce another portion of the input sentences to generate an improved prediction for the [MASK] token. We achieve this goal by only allowing identical or similar sentences that exceed a certain threshold as input to BERT. Similar inputs in our case are sentences that have a similarity to our [MASK] sentence. To be able to determine a similarity score, we need the vector of the [MASK] sentence and the vector of an input sentence. Using cosine similarity, a similarity value is calculated between the [MASK] sentence and the input sentence. This method is repeated for each input sentence found to determine a similarity score between -1 and 1 for each. Here -1 stands for no similarity, 0 for an independent sentence and 1 for an identical sentence. By using a predefined threshold, input records with insufficient similarity can be filtered out. This threshold is also called similarity_score (=sim_score) in BERTuality and should be a small value above 0 when using the extraction query.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271c93cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Query:\n",
      "-> Found a total of 39 input sentences.\n",
      "-> Further reduced the amount of sentences by 51.25 %\n"
     ]
    }
   ],
   "source": [
    "# call the similarity query to find similar sentences to our masked sentence\n",
    "\n",
    "similarity_query_tuple = BERTuality.similarity_filter(mask_sentence, extraction_query, sim_score=0.25, return_tuples=True)\n",
    "\n",
    "print(\"Similarity Query:\")\n",
    "print(f\"-> Found a total of {len(similarity_query_tuple)} input sentences.\")\n",
    "print(f\"-> Further reduced the amount of sentences by {100 - len(similarity_query_tuple) * 100 / len(extraction_query):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f967ac5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity Score</th>\n",
       "      <th>Similarity Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.267162</td>\n",
       "      <td>South Korean President Yoon Suk Yeol departed South Korea on Thursday for Tokyo to meet with Pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291097</td>\n",
       "      <td>The other toasted a close friendship with MoscowOn Tuesday Japan prime minister laid a wreath fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.283896</td>\n",
       "      <td>Japanese Prime Minister Fumio Kishida began a surprise visit to Ukraine early Tuesday hours afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.261317</td>\n",
       "      <td>College football Mexican cola and muffins United Kingdom prime minister has plenty to talk about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.437871</td>\n",
       "      <td>Japanese prime minister to show olidarity' with Ukraine in visit that coincides with Chinese lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.555677</td>\n",
       "      <td>This is a list of prime ministers of Japan from when the first Japanese prime minister Itō Hirob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.509661</td>\n",
       "      <td>32 prime ministers under the Meiji Constitution had a mandate from the Emperor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.510935</td>\n",
       "      <td>Currently the prime minister under the Constitution of Japan shall be designated from among the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.487961</td>\n",
       "      <td>The incumbent prime minister is Fumio Kishida.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.465420</td>\n",
       "      <td>Prime Miniser Kishida is Japan first postwar leader to enter a war zone.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Similarity Score  \\\n",
       "0           0.267162   \n",
       "1           0.291097   \n",
       "2           0.283896   \n",
       "3           0.261317   \n",
       "4           0.437871   \n",
       "..               ...   \n",
       "34          0.555677   \n",
       "35          0.509661   \n",
       "36          0.510935   \n",
       "37          0.487961   \n",
       "38          0.465420   \n",
       "\n",
       "                                                                                       Similarity Query  \n",
       "0   South Korean President Yoon Suk Yeol departed South Korea on Thursday for Tokyo to meet with Pri...  \n",
       "1   The other toasted a close friendship with MoscowOn Tuesday Japan prime minister laid a wreath fo...  \n",
       "2   Japanese Prime Minister Fumio Kishida began a surprise visit to Ukraine early Tuesday hours afte...  \n",
       "3   College football Mexican cola and muffins United Kingdom prime minister has plenty to talk about...  \n",
       "4   Japanese prime minister to show olidarity' with Ukraine in visit that coincides with Chinese lea...  \n",
       "..                                                                                                  ...  \n",
       "34  This is a list of prime ministers of Japan from when the first Japanese prime minister Itō Hirob...  \n",
       "35                      32 prime ministers under the Meiji Constitution had a mandate from the Emperor.  \n",
       "36  Currently the prime minister under the Constitution of Japan shall be designated from among the ...  \n",
       "37                                                       The incumbent prime minister is Fumio Kishida.  \n",
       "38                             Prime Miniser Kishida is Japan first postwar leader to enter a war zone.  \n",
       "\n",
       "[39 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df = pd.DataFrame(similarity_query_tuple, columns=[\"Similarity Score\", \"Similarity Query\"])\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18d3a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all sentences into a sperarate list - needed because we used return_tuples\n",
    "\n",
    "similarity_query = [i[1] for i in similarity_query_tuple]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4dac44",
   "metadata": {},
   "source": [
    "### 3.4.3 Focus Query\n",
    "\n",
    "<p style='text-align: justify;'>We introduce the focus query into the query module to remove contextually unimportant information from sentences that are too long. Since there is usually only one short core information in a sentence that we need for BERTuality, it is important to extract just this core information from the sentence. Any additional information beyond this core may have a negative impact on the prediction result. Therefore, the focus query uses the previously created keywords that must be included in our sentence. We also assume that the information to be predicted must occur near the keywords we are looking for.</p>\n",
    "\n",
    "\n",
    "<p style='text-align: justify;'>However, to separate irrelevant from relevant parts of the sentence, we first determine the first positions of each keyword in the sentence. This span between the found positions forms our core information. In the next step, we truncate all words outside this interval and classify them as irrelevant for now. As mentioned earlier, our relevant information is located near the keywords, which means that some of the information may be outside the core. Therefore, after the truncation process, we again append a part of the original sentence to our core information. We refer to this appended part as the padding of the focus query. This padding can be defined by a value of our choice and includes the number of words appended to the left and right.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24fd62de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# call the focus query to extract the core information from our input sentences\n",
    "\n",
    "focus_query = BERTuality.keyword_focus(similarity_query, keywords, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffe7e6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of characters in the Similarity Query: 5923\n",
      "Total sum of characters in the Focus Query:      2396\n",
      "\n",
      "We achieved a reduction of 3527 characters in 39 sentences!\n"
     ]
    }
   ],
   "source": [
    "sim_len = sum([len(i) for i in similarity_query])\n",
    "foc_len = sum([len(i) for i in focus_query])\n",
    "\n",
    "print(\"Total sum of characters in the Similarity Query:\", sim_len)\n",
    "print(\"Total sum of characters in the Focus Query:     \", foc_len)\n",
    "print(f\"\\nWe achieved a reduction of {sim_len - foc_len} characters in {len(focus_query)} sentences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f86ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Focus Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meet with prime minister fumio kishida the first such summit on japan soil in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moscowon tuesday japan prime minister laid a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>japan prime minister fumio kishida.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>united kingdom prime minister has plenty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>japan prime minister to show.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>list of prime ministers of japan from when the first japanese prime minister itō hirobumi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32 prime minister under the.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>currently the prime minister under the constitution of japan shall be.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>the incumbent prime minister is fumio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>prime miniser kishida is japan first postwar.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   Focus Query\n",
       "0               meet with prime minister fumio kishida the first such summit on japan soil in.\n",
       "1                                                moscowon tuesday japan prime minister laid a.\n",
       "2                                                          japan prime minister fumio kishida.\n",
       "3                                                    united kingdom prime minister has plenty.\n",
       "4                                                                japan prime minister to show.\n",
       "..                                                                                         ...\n",
       "34  list of prime ministers of japan from when the first japanese prime minister itō hirobumi.\n",
       "35                                                                32 prime minister under the.\n",
       "36                      currently the prime minister under the constitution of japan shall be.\n",
       "37                                                      the incumbent prime minister is fumio.\n",
       "38                                               prime miniser kishida is japan first postwar.\n",
       "\n",
       "[39 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following are the optimal input sentences for our predictions with BERT\n",
    "\n",
    "focus_df = pd.DataFrame(focus_query, columns=[\"Focus Query\"])\n",
    "focus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb357c",
   "metadata": {},
   "source": [
    "## 3.5 Predictions with BERTuality\n",
    "\n",
    "<p style='text-align: justify;'>Finally, the [MASK] sentence and the actuality-related input sentences created by the BERTuality pipeline are passed to BERT. Ideally, the selected input sentences contain the maximum relevant content of all sentences from the data source.</p>\n",
    "\n",
    "\n",
    "### 3.5.1 Input Prediction\n",
    "\n",
    "<p style='text-align: justify;'>By passing the inputs to the fill mask pipeline a prediction for the [MASK] token is generated. This may differ from the prediction without any additional input. The reason for this is the priming of BERT with the input sentences created from the BERTuality pipeline described in chapter 2.2. Therefore, throughout the BERTuality pipeline, our goal is to find optimally suitable input sentences. This is because the more similar the input sentence is to the [MASK] sentence, the higher the probability that the sentence also contains the correct information to be predicted. In order to be able to generate a prediction, the corresponding input sentences from the BERTuality pipeline are passed to BERT in addition to the [MASK] sentence in the optimal input form already explained.</p>\n",
    "\n",
    "<p style='text-align: justify;'>To catch the case of a non-optimal input sentence, so that no wrong prediction is risked, all found sentences are used as inputs for BERT. Thus, the [MASK] token is predicted once for each of the included input sentences.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df0e785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the make_predictions function is equal to the \"fill_mask\" pipeline from chapter 2, but also returns a DataFrame\n",
    "\n",
    "input_prediction = BERTuality.make_predictions(mask_sentence, focus_query, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b90602cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked</th>\n",
       "      <th>input</th>\n",
       "      <th>token1</th>\n",
       "      <th>score1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>meet with prime minister fumio kishida the first such summit on japan soil in.</td>\n",
       "      <td>abe</td>\n",
       "      <td>0.432130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>moscowon tuesday japan prime minister laid a.</td>\n",
       "      <td>who</td>\n",
       "      <td>0.477845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>japan prime minister fumio kishida.</td>\n",
       "      <td>he</td>\n",
       "      <td>0.422209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>united kingdom prime minister has plenty.</td>\n",
       "      <td>tanaka</td>\n",
       "      <td>0.211324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>japan prime minister to show.</td>\n",
       "      <td>who</td>\n",
       "      <td>0.815423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>list of prime ministers of japan from when the first japanese prime minister itō hirobumi.</td>\n",
       "      <td>ito</td>\n",
       "      <td>0.944046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>32 prime minister under the.</td>\n",
       "      <td>who</td>\n",
       "      <td>0.689149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>currently the prime minister under the constitution of japan shall be.</td>\n",
       "      <td>tanaka</td>\n",
       "      <td>0.283642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>the incumbent prime minister is fumio.</td>\n",
       "      <td>tanaka</td>\n",
       "      <td>0.448058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>prime miniser kishida is japan first postwar.</td>\n",
       "      <td>tanaka</td>\n",
       "      <td>0.201175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           masked  \\\n",
       "0   Prime minister [MASK] is the leader of Japan.   \n",
       "1   Prime minister [MASK] is the leader of Japan.   \n",
       "2   Prime minister [MASK] is the leader of Japan.   \n",
       "3   Prime minister [MASK] is the leader of Japan.   \n",
       "4   Prime minister [MASK] is the leader of Japan.   \n",
       "..                                            ...   \n",
       "34  Prime minister [MASK] is the leader of Japan.   \n",
       "35  Prime minister [MASK] is the leader of Japan.   \n",
       "36  Prime minister [MASK] is the leader of Japan.   \n",
       "37  Prime minister [MASK] is the leader of Japan.   \n",
       "38  Prime minister [MASK] is the leader of Japan.   \n",
       "\n",
       "                                                                                         input  \\\n",
       "0               meet with prime minister fumio kishida the first such summit on japan soil in.   \n",
       "1                                                moscowon tuesday japan prime minister laid a.   \n",
       "2                                                          japan prime minister fumio kishida.   \n",
       "3                                                    united kingdom prime minister has plenty.   \n",
       "4                                                                japan prime minister to show.   \n",
       "..                                                                                         ...   \n",
       "34  list of prime ministers of japan from when the first japanese prime minister itō hirobumi.   \n",
       "35                                                                32 prime minister under the.   \n",
       "36                      currently the prime minister under the constitution of japan shall be.   \n",
       "37                                                      the incumbent prime minister is fumio.   \n",
       "38                                               prime miniser kishida is japan first postwar.   \n",
       "\n",
       "    token1    score1  \n",
       "0      abe  0.432130  \n",
       "1      who  0.477845  \n",
       "2       he  0.422209  \n",
       "3   tanaka  0.211324  \n",
       "4      who  0.815423  \n",
       "..     ...       ...  \n",
       "34     ito  0.944046  \n",
       "35     who  0.689149  \n",
       "36  tanaka  0.283642  \n",
       "37  tanaka  0.448058  \n",
       "38  tanaka  0.201175  \n",
       "\n",
       "[39 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a look at the first 5 predictions of the input prediction\n",
    "\n",
    "input_prediction_df = input_prediction[[\"masked\", \"input\", \"token1\", \"score1\"]]\n",
    "input_prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a81608",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>To select the correct word from the list of predictions, the scores of the same word occurrences are summed. This is what we call sum_up_score and is consequently the decision criterion for the [MASK] token, where the word with the highest value is selected for the [MASK] token. Nevertheless, it is not possible for the input prediction to predict every single token for the [MASK] token.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12b12ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>max_score</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>sum_up_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who</td>\n",
       "      <td>10</td>\n",
       "      <td>0.815423</td>\n",
       "      <td>0.282997</td>\n",
       "      <td>0.557341</td>\n",
       "      <td>5.573406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he</td>\n",
       "      <td>10</td>\n",
       "      <td>0.424347</td>\n",
       "      <td>0.185562</td>\n",
       "      <td>0.347023</td>\n",
       "      <td>3.470225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abe</td>\n",
       "      <td>7</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.115956</td>\n",
       "      <td>0.337288</td>\n",
       "      <td>2.361017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanaka</td>\n",
       "      <td>7</td>\n",
       "      <td>0.448058</td>\n",
       "      <td>0.147092</td>\n",
       "      <td>0.255719</td>\n",
       "      <td>1.790031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>making</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979401</td>\n",
       "      <td>0.979401</td>\n",
       "      <td>0.979401</td>\n",
       "      <td>0.979401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Token  Frequency  max_score  min_score  mean_score  sum_up_score\n",
       "0     who         10   0.815423   0.282997    0.557341      5.573406\n",
       "1      he         10   0.424347   0.185562    0.347023      3.470225\n",
       "2     abe          7   0.484453   0.115956    0.337288      2.361017\n",
       "3  tanaka          7   0.448058   0.147092    0.255719      1.790031\n",
       "4  making          1   0.979401   0.979401    0.979401      0.979401"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you can have a look at a summary of the top predictions\n",
    "\n",
    "BERTuality.simple_pred_results(input_prediction).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf8d36",
   "metadata": {},
   "source": [
    "### 3.5.2 The Token Problem\n",
    "\n",
    "<p style='text-align: justify;'>The token problem and thus the cause of some incorrect predictions of BERTuality can be traced back to the limited vocabulary and the restriction to a single [MASK] token. This vocabulary consists of 30,000 WordPieces. WordPieces make it possible to represent a large number of complicated tokens with a relatively compact vocabulary. A token that is outside the vocabulary is formed by concatenating WordPieces. For example, \"kishida\" is tokenized by BERT to \"ki + ##shi + ##da\". To allow related WordPieces to be reassembled into a token, successive WordPieces are marked with a leading \"##\". The token problem of BERTuality refers to the fact that we are limited to one [MASK] token in our [MASK] sentence and do not know if and from how many WordPieces the searched token is formed. Consequently, the simple input prediction is limited to predicting tokens that consist of a single WordPiece.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35c1ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Kishida\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5627090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kishida get's tokenized into ['ki', '##shida']\n"
     ]
    }
   ],
   "source": [
    "# the default way of tokenizing\n",
    "\n",
    "print(example, \"get's tokenized into\", tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c86023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kishida get's tokenized into ['ki', '##shi', '##da']\n"
     ]
    }
   ],
   "source": [
    "# the more powerful tokenizer of bertuality which breaks down tokens into the smallest from of WordPieces\n",
    "\n",
    "print(example, \"get's tokenized into\", BERTuality.better_tokenizer(example, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e813b4",
   "metadata": {},
   "source": [
    "### 3.5.3 WordPiece-Prediction\n",
    "\n",
    "<p style='text-align: justify;'>The WordPiece prediction is an extension of the input prediction. This method solves the token problem of many texts and manages to make tokens that are not included in the vocabulary predictable. Tokens, which consist of several connected WordPieces, are called WordPiece-Token (=WPT) in the following for simplification. The WordPiece prediction uses a procedure of concatenated predictions of single WordPieces with the help of a block list. The block list contains modifications of the original input sentence. The modifications of an input sentence are created by inserting the individual WordPieces of the WPT one after the other at the original token position of the WPT of the input sentence. This results in a blocklist with a length n for the number of WordPieces of a WPT. The above mentioned block lists are created for each WPT of a sentence and then passed to BERT for prediction.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec6f74dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create blocklist for sentence: \"meet with prime minister fumio kishida the first such summit on japan soil in.\"\n",
      "Found positions of WPT: [[4, 5, 6], [7, 8, 9]]\n",
      "\n",
      "Block Nr. 0\n",
      "  WP-Position 4: fu\n",
      "  WP-Position 5: ##mi\n",
      "  WP-Position 6: ##o\n",
      "\n",
      "Block Nr. 1\n",
      "  WP-Position 7: ki\n",
      "  WP-Position 8: ##shi\n",
      "  WP-Position 9: ##da\n",
      "\n",
      "Sentences for Block Nr. 0\n",
      "  meet with prime minister fu the first such summit on japan soil in.\n",
      "  meet with prime minister mi the first such summit on japan soil in.\n",
      "  meet with prime minister o the first such summit on japan soil in.\n"
     ]
    }
   ],
   "source": [
    "# below you can find an example for a blocklist:\n",
    "\n",
    "sentence = focus_query[0]\n",
    "\n",
    "# 1. Step: tokenize the sentence:\n",
    "tokens = BERTuality.better_tokenizer(sentence, tokenizer)\n",
    "\n",
    "# 2. Step: find every WordPiece position:\n",
    "every_wp_position = BERTuality.wp_find_positions(tokens)\n",
    "\n",
    "# 3. Step: create the blocklist for the sentence\n",
    "wpw_input_sentences = BERTuality.wp_input_sentence_creator(tokens, every_wp_position, tokenizer)    \n",
    "\n",
    "print(f'Create blocklist for sentence: \"{sentence}\"')\n",
    "print('Found positions of WPT:', every_wp_position)\n",
    "\n",
    "for nr, i in enumerate(every_wp_position):\n",
    "    print(f'\\nBlock Nr. {nr}')\n",
    "    for j in i:\n",
    "        print(f'  WP-Position {j}: {tokens[j]}')\n",
    "        \n",
    "print('\\nSentences for Block Nr. 0')\n",
    "for i in wpw_input_sentences[0]:\n",
    "    print(' ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2932ce56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [02:53<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now let's move on to the predictions with WordPiece Prediction:\n",
    "\n",
    "wp_prediction = BERTuality.word_piece_prediction(mask_sentence, focus_query, model, tokenizer, combine=False, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "486d83c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked</th>\n",
       "      <th>input</th>\n",
       "      <th>token1</th>\n",
       "      <th>score1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>0    meet with prime minister fu the first such summit on japan soil in.\n",
       "1    meet with prime mi...</td>\n",
       "      <td>fumio</td>\n",
       "      <td>0.997911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>0     meet with prime minister ki the first such summit on japan soil in.\n",
       "1    meet with prime m...</td>\n",
       "      <td>kishida</td>\n",
       "      <td>0.996877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>meet with prime minister fumio kishida the first such summit on japan soil in.</td>\n",
       "      <td>abe</td>\n",
       "      <td>0.432130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>0    moscow tuesday japan prime minister laid a.\n",
       "1        on tuesday japan prime minister laid a...</td>\n",
       "      <td>who</td>\n",
       "      <td>0.146832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>moscowon tuesday japan prime minister laid a.</td>\n",
       "      <td>who</td>\n",
       "      <td>0.477845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>0    the incumbent prime minister is fu.\n",
       "1    the incumbent prime minister is mi.\n",
       "2     the incu...</td>\n",
       "      <td>fumio</td>\n",
       "      <td>0.926627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>the incumbent prime minister is fumio.</td>\n",
       "      <td>tanaka</td>\n",
       "      <td>0.448058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>0    prime mini is japan first postwar.\n",
       "1     prime ser is japan first postwar.\n",
       "Name: input, dty...</td>\n",
       "      <td>meijijapan</td>\n",
       "      <td>0.166201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>0     prime ki is japan first postwar.\n",
       "1    prime shi is japan first postwar.\n",
       "2     prime da is ...</td>\n",
       "      <td>kishitanaka</td>\n",
       "      <td>0.402761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Prime minister [MASK] is the leader of Japan.</td>\n",
       "      <td>prime miniser kishida is japan first postwar.</td>\n",
       "      <td>tanaka</td>\n",
       "      <td>0.201175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           masked  \\\n",
       "0   Prime minister [MASK] is the leader of Japan.   \n",
       "1   Prime minister [MASK] is the leader of Japan.   \n",
       "2   Prime minister [MASK] is the leader of Japan.   \n",
       "3   Prime minister [MASK] is the leader of Japan.   \n",
       "4   Prime minister [MASK] is the leader of Japan.   \n",
       "..                                            ...   \n",
       "88  Prime minister [MASK] is the leader of Japan.   \n",
       "89  Prime minister [MASK] is the leader of Japan.   \n",
       "90  Prime minister [MASK] is the leader of Japan.   \n",
       "91  Prime minister [MASK] is the leader of Japan.   \n",
       "92  Prime minister [MASK] is the leader of Japan.   \n",
       "\n",
       "                                                                                                  input  \\\n",
       "0   0    meet with prime minister fu the first such summit on japan soil in.\n",
       "1    meet with prime mi...   \n",
       "1   0     meet with prime minister ki the first such summit on japan soil in.\n",
       "1    meet with prime m...   \n",
       "2                        meet with prime minister fumio kishida the first such summit on japan soil in.   \n",
       "3   0    moscow tuesday japan prime minister laid a.\n",
       "1        on tuesday japan prime minister laid a...   \n",
       "4                                                         moscowon tuesday japan prime minister laid a.   \n",
       "..                                                                                                  ...   \n",
       "88  0    the incumbent prime minister is fu.\n",
       "1    the incumbent prime minister is mi.\n",
       "2     the incu...   \n",
       "89                                                               the incumbent prime minister is fumio.   \n",
       "90  0    prime mini is japan first postwar.\n",
       "1     prime ser is japan first postwar.\n",
       "Name: input, dty...   \n",
       "91  0     prime ki is japan first postwar.\n",
       "1    prime shi is japan first postwar.\n",
       "2     prime da is ...   \n",
       "92                                                        prime miniser kishida is japan first postwar.   \n",
       "\n",
       "         token1    score1  \n",
       "0         fumio  0.997911  \n",
       "1       kishida  0.996877  \n",
       "2           abe  0.432130  \n",
       "3           who  0.146832  \n",
       "4           who  0.477845  \n",
       "..          ...       ...  \n",
       "88        fumio  0.926627  \n",
       "89       tanaka  0.448058  \n",
       "90   meijijapan  0.166201  \n",
       "91  kishitanaka  0.402761  \n",
       "92       tanaka  0.201175  \n",
       "\n",
       "[93 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_prediction_df = wp_prediction[[\"masked\", \"input\", \"token1\", \"score1\"]]\n",
    "wp_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01870028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>max_score</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>sum_up_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kishida</td>\n",
       "      <td>15</td>\n",
       "      <td>0.997021</td>\n",
       "      <td>0.871143</td>\n",
       "      <td>0.981703</td>\n",
       "      <td>14.725542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fumio</td>\n",
       "      <td>15</td>\n",
       "      <td>0.997911</td>\n",
       "      <td>0.830478</td>\n",
       "      <td>0.976237</td>\n",
       "      <td>14.643549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who</td>\n",
       "      <td>13</td>\n",
       "      <td>0.815423</td>\n",
       "      <td>0.146832</td>\n",
       "      <td>0.542155</td>\n",
       "      <td>7.048012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he</td>\n",
       "      <td>11</td>\n",
       "      <td>0.424347</td>\n",
       "      <td>0.162035</td>\n",
       "      <td>0.330206</td>\n",
       "      <td>3.632261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abe</td>\n",
       "      <td>7</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.115956</td>\n",
       "      <td>0.337288</td>\n",
       "      <td>2.361017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token  Frequency  max_score  min_score  mean_score  sum_up_score\n",
       "0  kishida         15   0.997021   0.871143    0.981703     14.725542\n",
       "1    fumio         15   0.997911   0.830478    0.976237     14.643549\n",
       "2      who         13   0.815423   0.146832    0.542155      7.048012\n",
       "3       he         11   0.424347   0.162035    0.330206      3.632261\n",
       "4      abe          7   0.484453   0.115956    0.337288      2.361017"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_pred_simple = BERTuality.simple_pred_results(wp_prediction)\n",
    "wp_pred_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad0bb3",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>The WordPiece prediction algorithm allows each WPT to be mapped by individual predictions of the WordPieces. This way, a WordPiece inserted at the original position of the original WPT will still have the same context to the rest of the sentence due to the positional data within the embeddings.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "389f5cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted [MASK] Token:        kishida\n",
      "Final output for our Sentence: Prime minister kishida is the leader of Japan.\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at our final prediction\n",
    "\n",
    "print(\"Predicted [MASK] Token:       \", wp_pred_simple[\"Token\"][0])\n",
    "print(\"Final output for our Sentence:\", mask_sentence.replace(\"[MASK]\", wp_pred_simple[\"Token\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853a7e2",
   "metadata": {},
   "source": [
    "# 4. Result & Playground\n",
    "\n",
    "<p style='text-align: justify;'><b>Result:</b> BERTuality helps correct misinformation in the form of outdated information. Since the use of misinformation is a great danger, the application of BERTuality can be very useful. The obtained performance is a starting point for further development of the method using the presented approaches.</p>\n",
    "\n",
    "<p style='text-align: justify;'>Below this cell you can test BERTuality yourself. You can find an implemented default configuration, but it is also possible to define a custom configuration and play around. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5a6eb13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>bertuality/model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <td>bertuality/tokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_date</th>\n",
       "      <td>2023-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_date</th>\n",
       "      <td>2023-04-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use_NewsAPI</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use_guardian</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use_wikipedia</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset_size</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim_score</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focus_padding</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duplicates</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similarity</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focus</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_input</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_target_token</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Values\n",
       "model                  bertuality/model\n",
       "tokenizer          bertuality/tokenizer\n",
       "from_date                    2023-01-26\n",
       "to_date                      2023-04-26\n",
       "use_NewsAPI                        True\n",
       "use_guardian                       True\n",
       "use_wikipedia                      True\n",
       "subset_size                           2\n",
       "sim_score                          0.25\n",
       "focus_padding                         6\n",
       "duplicates                        False\n",
       "extraction                         True\n",
       "similarity                         True\n",
       "focus                              True\n",
       "max_input                            30\n",
       "threshold                           0.9\n",
       "only_target_token                  True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults = BERTuality_quickstart.load_default_config()\n",
    "values, keys = list(defaults.values()), list(defaults.keys())\n",
    "\n",
    "pd.DataFrame(values, keys, columns=[\"Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "790a3be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Load config -------> Done\n",
      "Step 2: Load latest data --> Done\n",
      "Step 3: Prepare data ------> Done\n",
      "Step 4: Start Prediction:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [01:12<00:00,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction: Dara Khosrowshahi is the CEO of Uber.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# In the default configuration, to_date is always set to today and from_day is set to 90 days in the past.  \n",
    "\n",
    "BERTuality_quickstart.bertuality(\"Dara Khosrowshahi is the CEO of [MASK].\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
